{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Measuring Clevrness: Black- Box Testing of Visual Reasoning Models"
      ],
      "metadata": {
        "id": "vYihWOHW8pQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, few visual QA models are programmed again to evaluate the confidence percentage of the reasoning capabilities exhibitted by the visual QA player as the adversarial player tries to fool the Visual QA player."
      ],
      "metadata": {
        "id": "XDsTLlcH893P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#  Compositional Attention Networks \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1bdV8QZPn7Sy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f3KRcszfTBTr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch import nn\n",
        "from torch.nn.init import kaiming_uniform_, xavier_uniform_, normal\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def linear(in_dim, out_dim, bias=True):\n",
        "    lin = nn.Linear(in_dim, out_dim, bias=bias)\n",
        "    xavier_uniform_(lin.weight)\n",
        "    if bias:\n",
        "        lin.bias.data.zero_()\n",
        "\n",
        "    return lin\n",
        "\n",
        "class ControlUnit(nn.Module):\n",
        "    def __init__(self, dim, max_step):\n",
        "        super().__init__()\n",
        "\n",
        "        self.position_aware = nn.ModuleList()\n",
        "        for i in range(max_step):\n",
        "            self.position_aware.append(linear(dim * 2, dim))\n",
        "\n",
        "        self.control_question = linear(dim * 2, dim)\n",
        "        self.attn = linear(dim, 1)\n",
        "\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, step, context, question, control):\n",
        "        position_aware = self.position_aware[step](question)\n",
        "\n",
        "        control_question = torch.cat([control, position_aware], 1)\n",
        "        control_question = self.control_question(control_question)\n",
        "        control_question = control_question.unsqueeze(1)\n",
        "\n",
        "        context_prod = control_question * context\n",
        "        attn_weight = self.attn(context_prod)\n",
        "\n",
        "        attn = F.softmax(attn_weight, 1)\n",
        "\n",
        "        next_control = (attn * context).sum(1)\n",
        "\n",
        "        return next_control\n",
        "\n",
        "\n",
        "class ReadUnit(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mem = linear(dim, dim)\n",
        "        self.concat = linear(dim * 2, dim)\n",
        "        self.attn = linear(dim, 1)\n",
        "\n",
        "    def forward(self, memory, know, control):\n",
        "        mem = self.mem(memory[-1]).unsqueeze(2)\n",
        "        concat = self.concat(torch.cat([mem * know, know], 1) \\\n",
        "                                .permute(0, 2, 1))\n",
        "        attn = concat * control[-1].unsqueeze(1)\n",
        "        attn = self.attn(attn).squeeze(2)\n",
        "        attn = F.softmax(attn, 1).unsqueeze(1)\n",
        "\n",
        "        read = (attn * know).sum(2)\n",
        "\n",
        "        return read\n",
        "\n",
        "\n",
        "class WriteUnit(nn.Module):\n",
        "    def __init__(self, dim, self_attention=False, memory_gate=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.concat = linear(dim * 2, dim)\n",
        "\n",
        "        if self_attention:\n",
        "            self.attn = linear(dim, 1)\n",
        "            self.mem = linear(dim, dim)\n",
        "\n",
        "        if memory_gate:\n",
        "            self.control = linear(dim, 1)\n",
        "\n",
        "        self.self_attention = self_attention\n",
        "        self.memory_gate = memory_gate\n",
        "\n",
        "    def forward(self, memories, retrieved, controls):\n",
        "        prev_mem = memories[-1]\n",
        "        concat = self.concat(torch.cat([retrieved, prev_mem], 1))\n",
        "        next_mem = concat\n",
        "\n",
        "        if self.self_attention:\n",
        "            controls_cat = torch.stack(controls[:-1], 2)\n",
        "            attn = controls[-1].unsqueeze(2) * controls_cat\n",
        "            attn = self.attn(attn.permute(0, 2, 1))\n",
        "            attn = F.softmax(attn, 1).permute(0, 2, 1)\n",
        "\n",
        "            memories_cat = torch.stack(memories, 2)\n",
        "            attn_mem = (attn * memories_cat).sum(2)\n",
        "            next_mem = self.mem(attn_mem) + concat\n",
        "\n",
        "        if self.memory_gate:\n",
        "            control = self.control(controls[-1])\n",
        "            gate = F.sigmoid(control)\n",
        "            next_mem = gate * prev_mem + (1 - gate) * next_mem\n",
        "\n",
        "        return next_mem\n",
        "\n",
        "\n",
        "class MACUnit(nn.Module):\n",
        "    def __init__(self, dim, max_step=12,\n",
        "                self_attention=False, memory_gate=False,\n",
        "                dropout=0.15):\n",
        "        super().__init__()\n",
        "\n",
        "        self.control = ControlUnit(dim, max_step)\n",
        "        self.read = ReadUnit(dim)\n",
        "        self.write = WriteUnit(dim, self_attention, memory_gate)\n",
        "\n",
        "        self.mem_0 = nn.Parameter(torch.zeros(1, dim))\n",
        "        self.control_0 = nn.Parameter(torch.zeros(1, dim))\n",
        "\n",
        "        self.dim = dim\n",
        "        self.max_step = max_step\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def get_mask(self, x, dropout):\n",
        "        mask = torch.empty_like(x).bernoulli_(1 - dropout)\n",
        "        mask = mask / (1 - dropout)\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def forward(self, context, question, knowledge):\n",
        "        b_size = question.size(0)\n",
        "\n",
        "        control = self.control_0.expand(b_size, self.dim)\n",
        "        memory = self.mem_0.expand(b_size, self.dim)\n",
        "\n",
        "        if self.training:\n",
        "            control_mask = self.get_mask(control, self.dropout)\n",
        "            memory_mask = self.get_mask(memory, self.dropout)\n",
        "            control = control * control_mask\n",
        "            memory = memory * memory_mask\n",
        "\n",
        "        controls = [control]\n",
        "        memories = [memory]\n",
        "\n",
        "        for i in range(self.max_step):\n",
        "            control = self.control(i, context, question, control)\n",
        "            if self.training:\n",
        "                control = control * control_mask\n",
        "            controls.append(control)\n",
        "\n",
        "            read = self.read(memories, knowledge, controls)\n",
        "            memory = self.write(memories, read, controls)\n",
        "            if self.training:\n",
        "                memory = memory * memory_mask\n",
        "            memories.append(memory)\n",
        "\n",
        "        return memory\n",
        "\n",
        "\n",
        "class MACNetwork(nn.Module):\n",
        "    def __init__(self, n_vocab, dim, embed_hidden=300,\n",
        "                max_step=12, self_attention=False, memory_gate=False,\n",
        "                classes=28, dropout=0.15):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(nn.Conv2d(1024, dim, 3, padding=1),\n",
        "                                nn.ELU(),\n",
        "                                nn.Conv2d(dim, dim, 3, padding=1),\n",
        "                                nn.ELU())\n",
        "\n",
        "        self.embed = nn.Embedding(n_vocab, embed_hidden)\n",
        "        self.lstm = nn.LSTM(embed_hidden, dim,\n",
        "                        batch_first=True, bidirectional=True)\n",
        "        self.lstm_proj = nn.Linear(dim * 2, dim)\n",
        "\n",
        "        self.mac = MACUnit(dim, max_step,\n",
        "                        self_attention, memory_gate, dropout)\n",
        "\n",
        "\n",
        "        self.classifier = nn.Sequential(linear(dim * 3, dim),\n",
        "                                        nn.ELU(),\n",
        "                                        linear(dim, classes))\n",
        "\n",
        "        self.max_step = max_step\n",
        "        self.dim = dim\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.embed.weight.data.uniform_(0, 1)\n",
        "\n",
        "        kaiming_uniform_(self.conv[0].weight)\n",
        "        self.conv[0].bias.data.zero_()\n",
        "        kaiming_uniform_(self.conv[2].weight)\n",
        "        self.conv[2].bias.data.zero_()\n",
        "\n",
        "        kaiming_uniform_(self.classifier[0].weight)\n",
        "\n",
        "    def forward(self, image, question, question_len, dropout=0.15):\n",
        "        b_size = question.size(0)\n",
        "\n",
        "        img = self.conv(image)\n",
        "        img = img.view(b_size, self.dim, -1)\n",
        "\n",
        "        embed = self.embed(question)\n",
        "        embed = nn.utils.rnn.pack_padded_sequence(embed, question_len,\n",
        "                                                batch_first=True)\n",
        "        lstm_out, (h, _) = self.lstm(embed)\n",
        "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out,\n",
        "                                                    batch_first=True)\n",
        "        lstm_out = self.lstm_proj(lstm_out)\n",
        "        h = h.permute(1, 0, 2).contiguous().view(b_size, -1)\n",
        "\n",
        "        memory = self.mac(lstm_out, h, img)\n",
        "\n",
        "        out = torch.cat([memory, h], 1)\n",
        "        out = self.classifier(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import h5py\n",
        "\n",
        "from transforms import Scale\n",
        "\n",
        "class CLEVR(Dataset):\n",
        "    def __init__(self, root, split='train', transform=None):\n",
        "        with open(f'data/{split}.pkl', 'rb') as f:\n",
        "            self.data = pickle.load(f)\n",
        "\n",
        "        # self.transform = transform\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "\n",
        "        self.h = h5py.File('data/{}_features.hdf5'.format(split), 'r')\n",
        "        self.img = self.h['data']\n",
        "\n",
        "    def close(self):\n",
        "        self.h.close()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        imgfile, question, answer, family = self.data[index]\n",
        "        # img = Image.open(os.path.join(self.root, 'images',\n",
        "        #                            self.split, imgfile)).convert('RGB')\n",
        "\n",
        "        # img = self.transform(img)\n",
        "        id = int(imgfile.rsplit('_', 1)[1][:-4])\n",
        "        img = torch.from_numpy(self.img[id])\n",
        "\n",
        "        return img, question, len(question), answer, family\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    Scale([224, 224]),\n",
        "    transforms.Pad(4),\n",
        "    transforms.RandomCrop([224, 224]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                        std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "def collate_data(batch):\n",
        "    images, lengths, answers, families = [], [], [], []\n",
        "    batch_size = len(batch)\n",
        "\n",
        "    max_len = max(map(lambda x: len(x[1]), batch))\n",
        "\n",
        "    questions = np.zeros((batch_size, max_len), dtype=np.int64)\n",
        "    sort_by_len = sorted(batch, key=lambda x: len(x[1]), reverse=True)\n",
        "\n",
        "    for i, b in enumerate(sort_by_len):\n",
        "        image, question, length, answer, family = b\n",
        "        images.append(image)\n",
        "        length = len(question)\n",
        "        questions[i, :length] = question\n",
        "        lengths.append(length)\n",
        "        answers.append(answer)\n",
        "        families.append(family)\n",
        "\n",
        "    return torch.stack(images), torch.from_numpy(questions), \\\n",
        "        lengths, torch.LongTensor(answers), families"
      ],
      "metadata": {
        "id": "8AlMIT4cTnv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding\n"
      ],
      "metadata": {
        "id": "rLFQGvojpbNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "MDETR model and criterion classes.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, Optional\n",
        "\n",
        "import torch\n",
        "import torch.distributed\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "import util.dist as dist\n",
        "from util import box_ops\n",
        "from util.metrics import accuracy\n",
        "from util.misc import NestedTensor, interpolate\n",
        "\n",
        "from .backbone import build_backbone\n",
        "from .matcher import build_matcher\n",
        "from .postprocessors import build_postprocessors\n",
        "from .segmentation import DETRsegm, dice_loss, sigmoid_focal_loss\n",
        "from .transformer import build_transformer\n",
        "\n",
        "\n",
        "class MDETR(nn.Module):\n",
        "    \"\"\" This is the MDETR module that performs modulated object detection \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone,\n",
        "        transformer,\n",
        "        num_classes,\n",
        "        num_queries,\n",
        "        aux_loss=False,\n",
        "        contrastive_hdim=64,\n",
        "        contrastive_loss=False,\n",
        "        contrastive_align_loss=False,\n",
        "        qa_dataset: Optional[str] = None,\n",
        "        split_qa_heads=True,\n",
        "        predict_final=False,\n",
        "    ):\n",
        "        \"\"\"Initializes the model.\n",
        "        Args:\n",
        "            backbone: torch module of the backbone to be used. See backbone.py\n",
        "            transformer: torch module of the transformer architecture. See transformer.py\n",
        "            num_classes: number of object classes\n",
        "            num_queries: number of object queries, ie detection slot. This is the maximal number of objects\n",
        "                         MDETR can detect in a single image. For COCO, we recommend 100 queries.\n",
        "            aux_loss: True if auxiliary decoding losses (loss at each decoder layer) are to be used.\n",
        "            contrastive_hdim: dimension used for projecting the embeddings before computing contrastive loss\n",
        "            contrastive_loss: If true, perform image-text contrastive learning\n",
        "            contrastive_align_loss: If true, perform box - token contrastive learning\n",
        "            qa_dataset: If not None, train a QA head for the target dataset (CLEVR or GQA)\n",
        "            split_qa_heads: If true, use several head for each question type\n",
        "            predict_final: If true, will predict if a given box is in the actual referred set.\n",
        "                           Useful for CLEVR-Ref+ only currently.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_queries = num_queries\n",
        "        self.transformer = transformer\n",
        "        hidden_dim = transformer.d_model\n",
        "        self.class_embed = nn.Linear(hidden_dim, num_classes + 1)\n",
        "        self.isfinal_embed = nn.Linear(hidden_dim, 1) if predict_final else None\n",
        "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
        "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
        "        if qa_dataset is not None:\n",
        "            nb_heads = 6 if qa_dataset == \"gqa\" else 4\n",
        "            self.qa_embed = nn.Embedding(nb_heads if split_qa_heads else 1, hidden_dim)\n",
        "\n",
        "        self.input_proj = nn.Conv2d(backbone.num_channels, hidden_dim, kernel_size=1)\n",
        "        self.backbone = backbone\n",
        "        self.aux_loss = aux_loss\n",
        "        self.contrastive_loss = contrastive_loss\n",
        "        if contrastive_loss:\n",
        "            self.contrastive_projection_image = nn.Linear(hidden_dim, contrastive_hdim, bias=False)\n",
        "            self.contrastive_projection_text = nn.Linear(\n",
        "                self.transformer.text_encoder.config.hidden_size, contrastive_hdim, bias=False\n",
        "            )\n",
        "        self.contrastive_align_loss = contrastive_align_loss\n",
        "        if contrastive_align_loss:\n",
        "            self.contrastive_align_projection_image = nn.Linear(hidden_dim, contrastive_hdim)\n",
        "            self.contrastive_align_projection_text = nn.Linear(hidden_dim, contrastive_hdim)\n",
        "\n",
        "        self.qa_dataset = qa_dataset\n",
        "        self.split_qa_heads = split_qa_heads\n",
        "        if qa_dataset is not None:\n",
        "            if split_qa_heads:\n",
        "                self.answer_type_head = nn.Linear(hidden_dim, 5)\n",
        "                # TODO: make this more general\n",
        "                if qa_dataset == \"gqa\":\n",
        "                    self.answer_rel_head = nn.Linear(hidden_dim, 1594)\n",
        "                    self.answer_obj_head = nn.Linear(hidden_dim, 3)\n",
        "                    self.answer_global_head = nn.Linear(hidden_dim, 111)\n",
        "                    self.answer_attr_head = nn.Linear(hidden_dim, 403)\n",
        "                    self.answer_cat_head = nn.Linear(hidden_dim, 678)\n",
        "                elif qa_dataset == \"clevr\":\n",
        "                    self.answer_type_head = nn.Linear(hidden_dim, 3)\n",
        "                    self.answer_binary_head = nn.Linear(hidden_dim, 1)\n",
        "                    self.answer_attr_head = nn.Linear(hidden_dim, 15)\n",
        "                    self.answer_reg_head = MLP(hidden_dim, hidden_dim, 20, 3)\n",
        "                else:\n",
        "                    assert False, f\"Invalid qa dataset {qa_dataset}\"\n",
        "            else:\n",
        "                # TODO: make this more general\n",
        "                assert qa_dataset == \"gqa\", \"Clevr QA is not supported with unified head\"\n",
        "                self.answer_head = nn.Linear(hidden_dim, 1853)\n",
        "\n",
        "    def forward(self, samples: NestedTensor, captions, encode_and_save=True, memory_cache=None):\n",
        "        \"\"\"The forward expects a NestedTensor, which consists of:\n",
        "           - samples.tensor: batched images, of shape [batch_size x 3 x H x W]\n",
        "           - samples.mask: a binary mask of shape [batch_size x H x W], containing 1 on padded pixels\n",
        "        It returns a dict with the following elements:\n",
        "           - \"pred_logits\": the classification logits (including no-object) for all queries.\n",
        "                            Shape= [batch_size x num_queries x (num_classes + 1)]\n",
        "           - \"pred_boxes\": The normalized boxes coordinates for all queries, represented as\n",
        "                           (center_x, center_y, height, width). These values are normalized in [0, 1],\n",
        "                           relative to the size of each individual image (disregarding possible padding).\n",
        "                           See PostProcess for information on how to retrieve the unnormalized bounding box.\n",
        "           - \"aux_outputs\": Optional, only returned when auxilary losses are activated. It is a list of\n",
        "                            dictionnaries containing the two above keys for each decoder layer.\n",
        "        \"\"\"\n",
        "        if not isinstance(samples, NestedTensor):\n",
        "            samples = NestedTensor.from_tensor_list(samples)\n",
        "\n",
        "        if encode_and_save:\n",
        "            assert memory_cache is None\n",
        "            features, pos = self.backbone(samples)\n",
        "            src, mask = features[-1].decompose()\n",
        "            query_embed = self.query_embed.weight\n",
        "            if self.qa_dataset is not None:\n",
        "                query_embed = torch.cat([query_embed, self.qa_embed.weight], 0)\n",
        "            memory_cache = self.transformer(\n",
        "                self.input_proj(src),\n",
        "                mask,\n",
        "                query_embed,\n",
        "                pos[-1],\n",
        "                captions,\n",
        "                encode_and_save=True,\n",
        "                text_memory=None,\n",
        "                img_memory=None,\n",
        "                text_attention_mask=None,\n",
        "            )\n",
        "\n",
        "            if self.contrastive_loss:\n",
        "                memory_cache[\"text_pooled_op\"] = self.contrastive_projection_text(memory_cache[\"text_pooled_op\"])\n",
        "                memory_cache[\"img_pooled_op\"] = self.contrastive_projection_image(memory_cache[\"img_pooled_op\"])\n",
        "\n",
        "            return memory_cache\n",
        "\n",
        "        else:\n",
        "            assert memory_cache is not None\n",
        "            hs = self.transformer(\n",
        "                mask=memory_cache[\"mask\"],\n",
        "                query_embed=memory_cache[\"query_embed\"],\n",
        "                pos_embed=memory_cache[\"pos_embed\"],\n",
        "                encode_and_save=False,\n",
        "                text_memory=memory_cache[\"text_memory_resized\"],\n",
        "                img_memory=memory_cache[\"img_memory\"],\n",
        "                text_attention_mask=memory_cache[\"text_attention_mask\"],\n",
        "            )\n",
        "            out = {}\n",
        "            if self.qa_dataset is not None:\n",
        "                if self.split_qa_heads:\n",
        "                    if self.qa_dataset == \"gqa\":\n",
        "                        answer_embeds = hs[0, :, -6:]\n",
        "                        hs = hs[:, :, :-6]\n",
        "                        out[\"pred_answer_type\"] = self.answer_type_head(answer_embeds[:, 0])\n",
        "                        out[\"pred_answer_obj\"] = self.answer_obj_head(answer_embeds[:, 1])\n",
        "                        out[\"pred_answer_rel\"] = self.answer_rel_head(answer_embeds[:, 2])\n",
        "                        out[\"pred_answer_attr\"] = self.answer_attr_head(answer_embeds[:, 3])\n",
        "                        out[\"pred_answer_cat\"] = self.answer_cat_head(answer_embeds[:, 4])\n",
        "                        out[\"pred_answer_global\"] = self.answer_global_head(answer_embeds[:, 5])\n",
        "                    elif self.qa_dataset == \"clevr\":\n",
        "                        answer_embeds = hs[0, :, -4:]\n",
        "                        hs = hs[:, :, :-4]\n",
        "                        out[\"pred_answer_type\"] = self.answer_type_head(answer_embeds[:, 0])\n",
        "                        out[\"pred_answer_binary\"] = self.answer_binary_head(answer_embeds[:, 1]).squeeze(-1)\n",
        "                        out[\"pred_answer_reg\"] = self.answer_reg_head(answer_embeds[:, 2])\n",
        "                        out[\"pred_answer_attr\"] = self.answer_attr_head(answer_embeds[:, 3])\n",
        "                    else:\n",
        "                        assert False, f\"Invalid qa dataset {self.qa_dataset}\"\n",
        "\n",
        "                else:\n",
        "                    answer_embeds = hs[0, :, -1]\n",
        "                    hs = hs[:, :, :-1]\n",
        "                    out[\"pred_answer\"] = self.answer_head(answer_embeds)\n",
        "\n",
        "            outputs_class = self.class_embed(hs)\n",
        "            outputs_coord = self.bbox_embed(hs).sigmoid()\n",
        "            out.update(\n",
        "                {\n",
        "                    \"pred_logits\": outputs_class[-1],\n",
        "                    \"pred_boxes\": outputs_coord[-1],\n",
        "                }\n",
        "            )\n",
        "            outputs_isfinal = None\n",
        "            if self.isfinal_embed is not None:\n",
        "                outputs_isfinal = self.isfinal_embed(hs)\n",
        "                out[\"pred_isfinal\"] = outputs_isfinal[-1]\n",
        "            proj_queries, proj_tokens = None, None\n",
        "            if self.contrastive_align_loss:\n",
        "                proj_queries = F.normalize(self.contrastive_align_projection_image(hs), p=2, dim=-1)\n",
        "                proj_tokens = F.normalize(\n",
        "                    self.contrastive_align_projection_text(memory_cache[\"text_memory\"]).transpose(0, 1), p=2, dim=-1\n",
        "                )\n",
        "                out.update(\n",
        "                    {\n",
        "                        \"proj_queries\": proj_queries[-1],\n",
        "                        \"proj_tokens\": proj_tokens,\n",
        "                        \"tokenized\": memory_cache[\"tokenized\"],\n",
        "                    }\n",
        "                )\n",
        "            if self.aux_loss:\n",
        "                if self.contrastive_align_loss:\n",
        "                    assert proj_tokens is not None and proj_queries is not None\n",
        "                    out[\"aux_outputs\"] = [\n",
        "                        {\n",
        "                            \"pred_logits\": a,\n",
        "                            \"pred_boxes\": b,\n",
        "                            \"proj_queries\": c,\n",
        "                            \"proj_tokens\": proj_tokens,\n",
        "                            \"tokenized\": memory_cache[\"tokenized\"],\n",
        "                        }\n",
        "                        for a, b, c in zip(outputs_class[:-1], outputs_coord[:-1], proj_queries[:-1])\n",
        "                    ]\n",
        "                else:\n",
        "                    out[\"aux_outputs\"] = [\n",
        "                        {\n",
        "                            \"pred_logits\": a,\n",
        "                            \"pred_boxes\": b,\n",
        "                        }\n",
        "                        for a, b in zip(outputs_class[:-1], outputs_coord[:-1])\n",
        "                    ]\n",
        "                if outputs_isfinal is not None:\n",
        "                    assert len(outputs_isfinal[:-1]) == len(out[\"aux_outputs\"])\n",
        "                    for i in range(len(outputs_isfinal[:-1])):\n",
        "                        out[\"aux_outputs\"][i][\"pred_isfinal\"] = outputs_isfinal[i]\n",
        "            return out\n",
        "\n",
        "\n",
        "class ContrastiveCriterion(nn.Module):\n",
        "    def __init__(self, temperature=0.1):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, pooled_text, pooled_image):\n",
        "\n",
        "        normalized_text_emb = F.normalize(pooled_text, p=2, dim=1)\n",
        "        normalized_img_emb = F.normalize(pooled_image, p=2, dim=1)\n",
        "\n",
        "        logits = torch.mm(normalized_img_emb, normalized_text_emb.t()) / self.temperature\n",
        "        labels = torch.arange(logits.size(0)).to(pooled_image.device)\n",
        "\n",
        "        loss_i = F.cross_entropy(logits, labels)\n",
        "        loss_t = F.cross_entropy(logits.t(), labels)\n",
        "        loss = (loss_i + loss_t) / 2.0\n",
        "        return loss\n",
        "\n",
        "\n",
        "class QACriterionGQA(nn.Module):\n",
        "    def __init__(self, split_qa_heads):\n",
        "        super().__init__()\n",
        "        self.split_qa_heads = split_qa_heads\n",
        "\n",
        "    def forward(self, output, answers):\n",
        "        loss = {}\n",
        "        if not self.split_qa_heads:\n",
        "            loss[\"loss_answer_total\"] = F.cross_entropy(output[\"pred_answer\"], answers[\"answer\"], reduction=\"mean\")\n",
        "            attr_total = (output[\"pred_answer\"].argmax(-1)) == answers[\"answer\"]\n",
        "            loss[\"accuracy_answer_total\"] = attr_total.float().mean()\n",
        "            return loss\n",
        "\n",
        "        device = output[\"pred_answer_type\"].device\n",
        "        loss[\"loss_answer_type\"] = F.cross_entropy(output[\"pred_answer_type\"], answers[\"answer_type\"])\n",
        "\n",
        "        type_acc = output[\"pred_answer_type\"].argmax(-1) == answers[\"answer_type\"]\n",
        "        loss[\"accuracy_answer_type\"] = type_acc.sum() / answers[\"answer_type\"].numel()\n",
        "\n",
        "        is_obj = answers[\"answer_type\"] == 0\n",
        "        is_attr = answers[\"answer_type\"] == 1\n",
        "        is_rel = answers[\"answer_type\"] == 2\n",
        "        is_global = answers[\"answer_type\"] == 3\n",
        "        is_cat = answers[\"answer_type\"] == 4\n",
        "\n",
        "        ## OBJ type\n",
        "        obj_norm = is_obj.sum() if is_obj.any() else 1.0\n",
        "        loss[\"loss_answer_obj\"] = (\n",
        "            F.cross_entropy(output[\"pred_answer_obj\"], answers[\"answer_obj\"], reduction=\"none\")\n",
        "            .masked_fill(~is_obj, 0)\n",
        "            .sum()\n",
        "            / obj_norm\n",
        "        )\n",
        "        obj_acc = (output[\"pred_answer_obj\"].argmax(-1)) == answers[\"answer_obj\"]\n",
        "        loss[\"accuracy_answer_obj\"] = (\n",
        "            obj_acc[is_obj].sum() / is_obj.sum() if is_obj.any() else torch.as_tensor(1.0, device=device)\n",
        "        )\n",
        "\n",
        "        ## ATTR type\n",
        "        attr_norm = is_attr.sum() if is_attr.any() else 1.0\n",
        "        loss[\"loss_answer_attr\"] = (\n",
        "            F.cross_entropy(output[\"pred_answer_attr\"], answers[\"answer_attr\"], reduction=\"none\")\n",
        "            .masked_fill(~is_attr, 0)\n",
        "            .sum()\n",
        "            / attr_norm\n",
        "        )\n",
        "        attr_acc = (output[\"pred_answer_attr\"].argmax(-1)) == answers[\"answer_attr\"]\n",
        "        loss[\"accuracy_answer_attr\"] = (\n",
        "            attr_acc[is_attr].sum() / is_attr.sum() if is_attr.any() else torch.as_tensor(1.0, device=device)\n",
        "        )\n",
        "\n",
        "        ## REL type\n",
        "        rel_norm = is_rel.sum() if is_rel.any() else 1.0\n",
        "        loss[\"loss_answer_rel\"] = (\n",
        "            F.cross_entropy(output[\"pred_answer_rel\"], answers[\"answer_rel\"], reduction=\"none\")\n",
        "            .masked_fill(~is_rel, 0)\n",
        "            .sum()\n",
        "            / rel_norm\n",
        "        )\n",
        "        rel_acc = (output[\"pred_answer_rel\"].argmax(-1)) == answers[\"answer_rel\"]\n",
        "        loss[\"accuracy_answer_rel\"] = (\n",
        "            rel_acc[is_rel].sum() / is_rel.sum() if is_rel.any() else torch.as_tensor(1.0, device=device)\n",
        "        )\n",
        "\n",
        "        ## GLOBAL type\n",
        "        global_norm = is_global.sum() if is_global.any() else 1.0\n",
        "        loss[\"loss_answer_global\"] = (\n",
        "            F.cross_entropy(output[\"pred_answer_global\"], answers[\"answer_global\"], reduction=\"none\")\n",
        "            .masked_fill(~is_global, 0)\n",
        "            .sum()\n",
        "            / global_norm\n",
        "        )\n",
        "        global_acc = (output[\"pred_answer_global\"].argmax(-1)) == answers[\"answer_global\"]\n",
        "        loss[\"accuracy_answer_global\"] = (\n",
        "            global_acc[is_global].sum() / is_global.sum() if is_global.any() else torch.as_tensor(1.0, device=device)\n",
        "        )\n",
        "\n",
        "        ## CAT type\n",
        "        cat_norm = is_cat.sum() if is_cat.any() else 1.0\n",
        "        loss[\"loss_answer_cat\"] = (\n",
        "            F.cross_entropy(output[\"pred_answer_cat\"], answers[\"answer_cat\"], reduction=\"none\")\n",
        "            .masked_fill(~is_cat, 0)\n",
        "            .sum()\n",
        "            / cat_norm\n",
        "        )\n",
        "        cat_acc = (output[\"pred_answer_cat\"].argmax(-1)) == answers[\"answer_cat\"]\n",
        "        loss[\"accuracy_answer_cat\"] = (\n",
        "            cat_acc[is_cat].sum() / is_cat.sum() if is_cat.any() else torch.as_tensor(1.0, device=device)\n",
        "        )\n",
        "\n",
        "        loss[\"accuracy_answer_total\"] = (\n",
        "            type_acc\n",
        "            * (is_obj * obj_acc + is_rel * rel_acc + is_attr * attr_acc + is_global * global_acc + is_cat * cat_acc)\n",
        "        ).sum() / type_acc.numel()\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class QACriterionClevr(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, output, answers):\n",
        "        loss = {}\n",
        "        loss[\"loss_answer_type\"] = F.cross_entropy(output[\"pred_answer_type\"], answers[\"answer_type\"])\n",
        "\n",
        "        type_acc = output[\"pred_answer_type\"].argmax(-1) == answers[\"answer_type\"]\n",
        "        loss[\"accuracy_answer_type\"] = type_acc.sum() / answers[\"answer_type\"].numel()\n",
        "\n",
        "        is_binary = answers[\"answer_type\"] == 0\n",
        "        is_attr = answers[\"answer_type\"] == 1\n",
        "        is_reg = answers[\"answer_type\"] == 2\n",
        "\n",
        "        binary_norm = is_binary.sum() if is_binary.any() else 1.0\n",
        "        loss[\"loss_answer_binary\"] = (\n",
        "            F.binary_cross_entropy_with_logits(output[\"pred_answer_binary\"], answers[\"answer_binary\"], reduction=\"none\")\n",
        "            .masked_fill(~is_binary, 0)\n",
        "            .sum()\n",
        "            / binary_norm\n",
        "        )\n",
        "        bin_acc = (output[\"pred_answer_binary\"].sigmoid() > 0.5) == answers[\"answer_binary\"]\n",
        "        loss[\"accuracy_answer_binary\"] = (\n",
        "            bin_acc[is_binary].sum() / is_binary.sum() if is_binary.any() else torch.as_tensor(1.0)\n",
        "        )\n",
        "\n",
        "        reg_norm = is_reg.sum() if is_reg.any() else 1.0\n",
        "        loss[\"loss_answer_reg\"] = (\n",
        "            F.cross_entropy(output[\"pred_answer_reg\"], answers[\"answer_reg\"], reduction=\"none\")\n",
        "            .masked_fill(~is_reg, 0)\n",
        "            .sum()\n",
        "            / reg_norm\n",
        "        )\n",
        "        reg_acc = (output[\"pred_answer_reg\"].argmax(-1)) == answers[\"answer_reg\"]\n",
        "        loss[\"accuracy_answer_reg\"] = reg_acc[is_reg].sum() / is_reg.sum() if is_reg.any() else torch.as_tensor(1.0)\n",
        "\n",
        "        attr_norm = is_attr.sum() if is_attr.any() else 1.0\n",
        "        loss[\"loss_answer_attr\"] = (\n",
        "            F.cross_entropy(output[\"pred_answer_attr\"], answers[\"answer_attr\"], reduction=\"none\")\n",
        "            .masked_fill(~is_attr, 0)\n",
        "            .sum()\n",
        "            / attr_norm\n",
        "        )\n",
        "        attr_acc = (output[\"pred_answer_attr\"].argmax(-1)) == answers[\"answer_attr\"]\n",
        "        loss[\"accuracy_answer_attr\"] = (\n",
        "            attr_acc[is_attr].sum() / is_attr.sum() if is_attr.any() else torch.as_tensor(1.0)\n",
        "        )\n",
        "\n",
        "        loss[\"accuracy_answer_total\"] = (\n",
        "            type_acc * (is_binary * bin_acc + is_reg * reg_acc + is_attr * attr_acc)\n",
        "        ).sum() / type_acc.numel()\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class SetCriterion(nn.Module):\n",
        "    \"\"\"This class computes the loss for DETR.\n",
        "    The process happens in two steps:\n",
        "        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n",
        "        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, matcher, eos_coef, losses, temperature):\n",
        "        \"\"\"Create the criterion.\n",
        "        Parameters:\n",
        "            num_classes: number of object categories, omitting the special no-object category\n",
        "            matcher: module able to compute a matching between targets and proposals\n",
        "            eos_coef: relative classification weight applied to the no-object category\n",
        "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.matcher = matcher\n",
        "        self.eos_coef = eos_coef\n",
        "        self.losses = losses\n",
        "        self.temperature = temperature\n",
        "        empty_weight = torch.ones(self.num_classes + 1)\n",
        "        empty_weight[-1] = self.eos_coef\n",
        "        self.register_buffer(\"empty_weight\", empty_weight)\n",
        "\n",
        "    def loss_isfinal(self, outputs, targets, positive_map, indices, num_boxes):\n",
        "        \"\"\"This loss is used in some referring expression dataset (specifically Clevr-REF+)\n",
        "        It trains the model to predict which boxes are being referred to (ie are \"final\")\n",
        "        Eg if the caption is \"the cube next to the cylinder\", MDETR will detect both the cube and the cylinder.\n",
        "        However, the cylinder is an intermediate reasoning step, only the cube is being referred here.\n",
        "        \"\"\"\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        src_isfinal = outputs[\"pred_isfinal\"][idx].squeeze(-1)\n",
        "        target_isfinal = torch.cat([t[\"isfinal\"][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
        "\n",
        "        loss_isfinal = F.binary_cross_entropy_with_logits(src_isfinal, target_isfinal, reduction=\"none\")\n",
        "\n",
        "        losses = {}\n",
        "        losses[\"loss_isfinal\"] = loss_isfinal.sum() / num_boxes\n",
        "        acc = (src_isfinal.sigmoid() > 0.5) == (target_isfinal > 0.5)\n",
        "        if acc.numel() == 0:\n",
        "            acc = acc.sum()\n",
        "        else:\n",
        "            acc = acc.float().mean()\n",
        "        losses[\"accuracy_isfinal\"] = acc\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def loss_labels(self, outputs, targets, positive_map, indices, num_boxes):\n",
        "        \"\"\"Classification loss (NLL)\n",
        "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
        "        \"\"\"\n",
        "\n",
        "        logits = outputs[\"pred_logits\"].log_softmax(-1)  # BS x (num_queries) x (num_tokens)\n",
        "\n",
        "        src_idx = self._get_src_permutation_idx(indices)\n",
        "        tgt_idx = []\n",
        "        offset = 0\n",
        "        for i, (_, tgt) in enumerate(indices):\n",
        "            tgt_idx.append(tgt + offset)\n",
        "            offset += len(targets[i][\"boxes\"])\n",
        "        tgt_idx = torch.cat(tgt_idx)\n",
        "\n",
        "        tgt_pos = positive_map[tgt_idx]\n",
        "        target_sim = torch.zeros_like(logits)\n",
        "        target_sim[:, :, -1] = 1\n",
        "        target_sim[src_idx] = tgt_pos\n",
        "\n",
        "        loss_ce = -(logits * target_sim).sum(-1)\n",
        "\n",
        "        eos_coef = torch.full(loss_ce.shape, self.eos_coef, device=target_sim.device)\n",
        "        eos_coef[src_idx] = 1\n",
        "\n",
        "        loss_ce = loss_ce * eos_coef\n",
        "        loss_ce = loss_ce.sum() / num_boxes\n",
        "\n",
        "        losses = {\"loss_ce\": loss_ce}\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def loss_contrastive_align(self, outputs, targets, positive_map, indices, num_boxes):\n",
        "        bs = outputs[\"proj_queries\"].shape[0]\n",
        "        tokenized = outputs[\"tokenized\"]\n",
        "\n",
        "        normalized_text_emb = outputs[\"proj_tokens\"]  # BS x (num_tokens) x hdim\n",
        "        normalized_img_emb = outputs[\"proj_queries\"]  # BS x (num_queries) x hdim\n",
        "\n",
        "        logits = (\n",
        "            torch.matmul(normalized_img_emb, normalized_text_emb.transpose(-1, -2)) / self.temperature\n",
        "        )  # BS x (num_queries) x (num_tokens)\n",
        "\n",
        "        # construct a map such that positive_map[k, i,j] = True iff query i is associated to token j in batch item k\n",
        "        # For efficency, the construction happens on CPU, then the whole matrix is transferred to GPU in one go.\n",
        "        positive_map = torch.zeros(logits.shape, dtype=torch.bool)\n",
        "        for i, ((idx_src, idx_tgt), tgt) in enumerate(zip(indices, targets)):\n",
        "            if \"tokens_positive\" in tgt:\n",
        "                cur_tokens = [tgt[\"tokens_positive\"][j] for j in idx_tgt]\n",
        "            else:\n",
        "                cur_tokens = [tgt[\"tokens\"][j] for j in idx_tgt]\n",
        "\n",
        "            for j, tok_list in enumerate(cur_tokens):\n",
        "                for (beg, end) in tok_list:\n",
        "                    beg_pos = tokenized.char_to_token(i, beg)\n",
        "                    end_pos = tokenized.char_to_token(i, end - 1)\n",
        "                    if beg_pos is None:\n",
        "                        try:\n",
        "                            beg_pos = tokenized.char_to_token(beg + 1)\n",
        "                            if beg_pos is None:\n",
        "                                beg_pos = tokenized.char_to_token(beg + 2)\n",
        "                        except:\n",
        "                            beg_pos = None\n",
        "                    if end_pos is None:\n",
        "                        try:\n",
        "                            end_pos = tokenized.char_to_token(end - 2)\n",
        "                            if end_pos is None:\n",
        "                                end_pos = tokenized.char_to_token(end - 3)\n",
        "                        except:\n",
        "                            end_pos = None\n",
        "                    if beg_pos is None or end_pos is None:\n",
        "                        continue\n",
        "\n",
        "                    assert beg_pos is not None and end_pos is not None\n",
        "                    positive_map[i, idx_src[j], beg_pos : end_pos + 1].fill_(True)\n",
        "\n",
        "        positive_map = positive_map.to(logits.device)\n",
        "        positive_logits = -logits.masked_fill(~positive_map, 0)\n",
        "        negative_logits = logits  # .masked_fill(positive_map, -1000000)\n",
        "\n",
        "        boxes_with_pos = positive_map.any(2)\n",
        "        pos_term = positive_logits.sum(2)\n",
        "        neg_term = negative_logits.logsumexp(2)\n",
        "\n",
        "        nb_pos = positive_map.sum(2) + 1e-6\n",
        "\n",
        "        box_to_token_loss = ((pos_term / nb_pos + neg_term)).masked_fill(~boxes_with_pos, 0).sum()\n",
        "\n",
        "        tokens_with_pos = positive_map.any(1)\n",
        "        pos_term = positive_logits.sum(1)\n",
        "        neg_term = negative_logits.logsumexp(1)\n",
        "\n",
        "        nb_pos = positive_map.sum(1) + 1e-6\n",
        "\n",
        "        tokens_to_boxes_loss = ((pos_term / nb_pos + neg_term)).masked_fill(~tokens_with_pos, 0).sum()\n",
        "        tot_loss = (box_to_token_loss + tokens_to_boxes_loss) / 2\n",
        "\n",
        "        return {\"loss_contrastive_align\": tot_loss / num_boxes}\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def loss_cardinality(self, outputs, targets, positive_map, indices, num_boxes):\n",
        "        \"\"\"Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n",
        "        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n",
        "        \"\"\"\n",
        "        pred_logits = outputs[\"pred_logits\"]\n",
        "        device = pred_logits.device\n",
        "        tgt_lengths = torch.as_tensor([len(v[\"labels\"]) for v in targets], device=device)\n",
        "        ## Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
        "        # normalized_text_emb = outputs[\"proj_tokens\"]  # BS x (num_tokens) x hdim\n",
        "        # normalized_img_emb = outputs[\"proj_queries\"]  # BS x (num_queries) x hdim\n",
        "\n",
        "        # logits = torch.matmul(\n",
        "        #    normalized_img_emb, normalized_text_emb.transpose(-1, -2)\n",
        "        # )  # BS x (num_queries) x (num_tokens)\n",
        "        # card_pred = (logits[:, :, 0] > 0.5).sum(1)\n",
        "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
        "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
        "        losses = {\"cardinality_error\": card_err}\n",
        "        return losses\n",
        "\n",
        "    def loss_boxes(self, outputs, targets, positive_map, indices, num_boxes):\n",
        "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n",
        "        targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n",
        "        The target boxes are expected in format (center_x, center_y, h, w), normalized by the image size.\n",
        "        \"\"\"\n",
        "        assert \"pred_boxes\" in outputs\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        src_boxes = outputs[\"pred_boxes\"][idx]\n",
        "        target_boxes = torch.cat([t[\"boxes\"][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
        "\n",
        "        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction=\"none\")\n",
        "\n",
        "        losses = {}\n",
        "        losses[\"loss_bbox\"] = loss_bbox.sum() / num_boxes\n",
        "\n",
        "        loss_giou = 1 - torch.diag(\n",
        "            box_ops.generalized_box_iou(box_ops.box_cxcywh_to_xyxy(src_boxes), box_ops.box_cxcywh_to_xyxy(target_boxes))\n",
        "        )\n",
        "        losses[\"loss_giou\"] = loss_giou.sum() / num_boxes\n",
        "        return losses\n",
        "\n",
        "    def loss_masks(self, outputs, targets, positive_map, indices, num_boxes):\n",
        "        \"\"\"Compute the losses related to the masks: the focal loss and the dice loss.\n",
        "        targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w]\n",
        "        \"\"\"\n",
        "        assert \"pred_masks\" in outputs\n",
        "\n",
        "        src_idx = self._get_src_permutation_idx(indices)\n",
        "        tgt_idx = self._get_tgt_permutation_idx(indices)\n",
        "\n",
        "        src_masks = outputs[\"pred_masks\"]\n",
        "\n",
        "        # TODO use valid to mask invalid areas due to padding in loss\n",
        "        target_masks, valid = NestedTensor.from_tensor_list([t[\"masks\"] for t in targets]).decompose()\n",
        "        target_masks = target_masks.to(src_masks)\n",
        "\n",
        "        src_masks = src_masks[src_idx]\n",
        "        # upsample predictions to the target size\n",
        "        src_masks = interpolate(src_masks[:, None], size=target_masks.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "        src_masks = src_masks[:, 0].flatten(1)\n",
        "\n",
        "        target_masks = target_masks[tgt_idx].flatten(1)\n",
        "\n",
        "        losses = {\n",
        "            \"loss_mask\": sigmoid_focal_loss(src_masks, target_masks, num_boxes),\n",
        "            \"loss_dice\": dice_loss(src_masks, target_masks, num_boxes),\n",
        "        }\n",
        "        return losses\n",
        "\n",
        "    def _get_src_permutation_idx(self, indices):\n",
        "        # permute predictions following indices\n",
        "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    def _get_tgt_permutation_idx(self, indices):\n",
        "        # permute targets following indices\n",
        "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
        "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
        "        return batch_idx, tgt_idx\n",
        "\n",
        "    def get_loss(self, loss, outputs, targets, positive_map, indices, num_boxes, **kwargs):\n",
        "        loss_map = {\n",
        "            \"labels\": self.loss_labels,\n",
        "            \"cardinality\": self.loss_cardinality,\n",
        "            \"boxes\": self.loss_boxes,\n",
        "            \"masks\": self.loss_masks,\n",
        "            \"isfinal\": self.loss_isfinal,\n",
        "            \"contrastive_align\": self.loss_contrastive_align,\n",
        "        }\n",
        "        assert loss in loss_map, f\"do you really want to compute {loss} loss?\"\n",
        "        return loss_map[loss](outputs, targets, positive_map, indices, num_boxes, **kwargs)\n",
        "\n",
        "    def forward(self, outputs, targets, positive_map):\n",
        "        \"\"\"This performs the loss computation.\n",
        "        Parameters:\n",
        "             outputs: dict of tensors, see the output specification of the model for the format\n",
        "             targets: list of dicts, such that len(targets) == batch_size.\n",
        "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
        "        \"\"\"\n",
        "        outputs_without_aux = {k: v for k, v in outputs.items() if k != \"aux_outputs\"}\n",
        "\n",
        "        # Retrieve the matching between the outputs of the last layer and the targets\n",
        "        indices = self.matcher(outputs_without_aux, targets, positive_map)\n",
        "\n",
        "        # Compute the average number of target boxes accross all nodes, for normalization purposes\n",
        "        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
        "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
        "        if dist.is_dist_avail_and_initialized():\n",
        "            torch.distributed.all_reduce(num_boxes)\n",
        "        num_boxes = torch.clamp(num_boxes / dist.get_world_size(), min=1).item()\n",
        "\n",
        "        # Compute all the requested losses\n",
        "        losses = {}\n",
        "        for loss in self.losses:\n",
        "            losses.update(self.get_loss(loss, outputs, targets, positive_map, indices, num_boxes))\n",
        "\n",
        "        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n",
        "        if \"aux_outputs\" in outputs:\n",
        "            for i, aux_outputs in enumerate(outputs[\"aux_outputs\"]):\n",
        "                indices = self.matcher(aux_outputs, targets, positive_map)\n",
        "                for loss in self.losses:\n",
        "                    if loss == \"masks\":\n",
        "                        # Intermediate masks losses are too costly to compute, we ignore them.\n",
        "                        continue\n",
        "                    kwargs = {}\n",
        "                    l_dict = self.get_loss(loss, aux_outputs, targets, positive_map, indices, num_boxes, **kwargs)\n",
        "                    l_dict = {k + f\"_{i}\": v for k, v in l_dict.items()}\n",
        "                    losses.update(l_dict)\n",
        "\n",
        "        return losses\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        h = [hidden_dim] * (num_layers - 1)\n",
        "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def build(args):\n",
        "    num_classes = 255\n",
        "    device = torch.device(args.device)\n",
        "\n",
        "    assert not args.masks or args.mask_model != \"none\"\n",
        "\n",
        "    qa_dataset = None\n",
        "    if args.do_qa:\n",
        "        assert not (\n",
        "            (\"clevr\" in args.combine_datasets or \"clevr_question\" in args.combine_datasets)\n",
        "            and \"gqa\" in args.combine_datasets\n",
        "        ), \"training GQA and CLEVR simultaneously is not supported\"\n",
        "        assert (\n",
        "            \"clevr_question\" in args.combine_datasets\n",
        "            or \"clevr\" in args.combine_datasets\n",
        "            or \"gqa\" in args.combine_datasets\n",
        "        ), \"Question answering require either gqa or clevr dataset\"\n",
        "        qa_dataset = \"gqa\" if \"gqa\" in args.combine_datasets else \"clevr\"\n",
        "\n",
        "    backbone = build_backbone(args)\n",
        "\n",
        "    transformer = build_transformer(args)\n",
        "\n",
        "    model = MDETR(\n",
        "        backbone,\n",
        "        transformer,\n",
        "        num_classes=num_classes,\n",
        "        num_queries=args.num_queries,\n",
        "        aux_loss=args.aux_loss,\n",
        "        contrastive_hdim=args.contrastive_loss_hdim,\n",
        "        contrastive_loss=args.contrastive_loss,\n",
        "        contrastive_align_loss=args.contrastive_align_loss,\n",
        "        qa_dataset=qa_dataset,\n",
        "        split_qa_heads=args.split_qa_heads,\n",
        "        predict_final=args.predict_final,\n",
        "    )\n",
        "    if args.mask_model != \"none\":\n",
        "        model = DETRsegm(\n",
        "            model,\n",
        "            mask_head=args.mask_model,\n",
        "            freeze_detr=(args.frozen_weights is not None),\n",
        "        )\n",
        "    matcher = build_matcher(args)\n",
        "    weight_dict = {\"loss_ce\": args.ce_loss_coef, \"loss_bbox\": args.bbox_loss_coef}\n",
        "    if args.contrastive_loss:\n",
        "        weight_dict[\"contrastive_loss\"] = args.contrastive_loss_coef\n",
        "    if args.contrastive_align_loss:\n",
        "        weight_dict[\"loss_contrastive_align\"] = args.contrastive_align_loss_coef\n",
        "    if args.predict_final:\n",
        "        weight_dict[\"loss_isfinal\"] = 1\n",
        "\n",
        "    weight_dict[\"loss_giou\"] = args.giou_loss_coef\n",
        "    if args.masks:\n",
        "        weight_dict[\"loss_mask\"] = args.mask_loss_coef\n",
        "        weight_dict[\"loss_dice\"] = args.dice_loss_coef\n",
        "\n",
        "    if args.do_qa:\n",
        "        if args.split_qa_heads:\n",
        "            weight_dict[\"loss_answer_type\"] = 1 * args.qa_loss_coef\n",
        "            if qa_dataset == \"gqa\":\n",
        "                weight_dict[\"loss_answer_cat\"] = 1 * args.qa_loss_coef\n",
        "                weight_dict[\"loss_answer_attr\"] = 1 * args.qa_loss_coef\n",
        "                weight_dict[\"loss_answer_rel\"] = 1 * args.qa_loss_coef\n",
        "                weight_dict[\"loss_answer_obj\"] = 1 * args.qa_loss_coef\n",
        "                weight_dict[\"loss_answer_global\"] = 1 * args.qa_loss_coef\n",
        "            else:\n",
        "                weight_dict[\"loss_answer_binary\"] = 1\n",
        "                weight_dict[\"loss_answer_attr\"] = 1\n",
        "                weight_dict[\"loss_answer_reg\"] = 1\n",
        "\n",
        "        else:\n",
        "            weight_dict[\"loss_answer_total\"] = 1 * args.qa_loss_coef\n",
        "\n",
        "    # TODO this is a hack\n",
        "    if args.aux_loss:\n",
        "        aux_weight_dict = {}\n",
        "        for i in range(args.dec_layers - 1):\n",
        "            aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n",
        "        weight_dict.update(aux_weight_dict)\n",
        "\n",
        "    losses = [\"labels\", \"boxes\", \"cardinality\"]\n",
        "    if args.masks:\n",
        "        losses += [\"masks\"]\n",
        "    if args.predict_final:\n",
        "        losses += [\"isfinal\"]\n",
        "    if args.contrastive_align_loss:\n",
        "        losses += [\"contrastive_align\"]\n",
        "\n",
        "    criterion = None\n",
        "    if not args.no_detection:\n",
        "        criterion = SetCriterion(\n",
        "            num_classes,\n",
        "            matcher=matcher,\n",
        "            eos_coef=args.eos_coef,\n",
        "            losses=losses,\n",
        "            temperature=args.temperature_NCE,\n",
        "        )\n",
        "        criterion.to(device)\n",
        "\n",
        "    if args.contrastive_loss:\n",
        "        contrastive_criterion = ContrastiveCriterion(temperature=args.temperature_NCE)\n",
        "        contrastive_criterion.to(device)\n",
        "    else:\n",
        "        contrastive_criterion = None\n",
        "\n",
        "    if args.do_qa:\n",
        "        if qa_dataset == \"gqa\":\n",
        "            qa_criterion = QACriterionGQA(split_qa_heads=args.split_qa_heads)\n",
        "        elif qa_dataset == \"clevr\":\n",
        "            qa_criterion = QACriterionClevr()\n",
        "        else:\n",
        "            assert False, f\"Invalid qa dataset {qa_dataset}\"\n",
        "        qa_criterion.to(device)\n",
        "    else:\n",
        "        qa_criterion = None\n",
        "    return model, criterion, contrastive_criterion, qa_criterion, weight_dict\n",
        "\n"
      ],
      "metadata": {
        "id": "U0bXOuQ6VPav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DETR Transformer class.\n",
        "\"\"\n",
        "Copy-paste from torch.nn.Transformer with modifications:\n",
        "    * positional encodings are passed in MHattention\n",
        "    * extra LN at the end of encoder is removed\n",
        "    * decoder returns a stack of activations from all decoding layers\n",
        "\"\"\n",
        "\n",
        "import copy\n",
        "from typing import List, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor, nn\n",
        "from transformers import RobertaModel, RobertaTokenizerFast\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model=512,\n",
        "        nhead=8,\n",
        "        num_encoder_layers=6,\n",
        "        num_decoder_layers=6,\n",
        "        dim_feedforward=2048,\n",
        "        dropout=0.1,\n",
        "        activation=\"relu\",\n",
        "        normalize_before=False,\n",
        "        return_intermediate_dec=False,\n",
        "        pass_pos_and_query=True,\n",
        "        text_encoder_type=\"roberta-base\",\n",
        "        freeze_text_encoder=False,\n",
        "        contrastive_loss=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pass_pos_and_query = pass_pos_and_query\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation, normalize_before)\n",
        "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
        "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
        "\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation, normalize_before)\n",
        "        decoder_norm = nn.LayerNorm(d_model)\n",
        "        self.decoder = TransformerDecoder(\n",
        "            decoder_layer, num_decoder_layers, decoder_norm, return_intermediate=return_intermediate_dec\n",
        "        )\n",
        "\n",
        "        self.CLS = nn.Embedding(1, d_model) if contrastive_loss else None\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "        self.tokenizer = RobertaTokenizerFast.from_pretrained(text_encoder_type)\n",
        "        self.text_encoder = RobertaModel.from_pretrained(text_encoder_type)\n",
        "\n",
        "        if freeze_text_encoder:\n",
        "            for p in self.text_encoder.parameters():\n",
        "                p.requires_grad_(False)\n",
        "\n",
        "        self.expander_dropout = 0.1\n",
        "        config = self.text_encoder.config\n",
        "        self.resizer = FeatureResizer(\n",
        "            input_feat_size=config.hidden_size,\n",
        "            output_feat_size=d_model,\n",
        "            dropout=self.expander_dropout,\n",
        "        )\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src=None,\n",
        "        mask=None,\n",
        "        query_embed=None,\n",
        "        pos_embed=None,\n",
        "        text=None,\n",
        "        encode_and_save=True,\n",
        "        text_memory=None,\n",
        "        img_memory=None,\n",
        "        text_attention_mask=None,\n",
        "    ):\n",
        "        if encode_and_save:\n",
        "            # flatten NxCxHxW to HWxNxC\n",
        "            bs, c, h, w = src.shape\n",
        "            src = src.flatten(2).permute(2, 0, 1)\n",
        "            device = src.device\n",
        "            pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "            query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
        "            mask = mask.flatten(1)\n",
        "\n",
        "            if self.CLS is not None:\n",
        "                # We add a CLS token to the image, to be used for contrastive loss\n",
        "\n",
        "                CLS = self.CLS.weight.view(1, 1, -1).repeat(1, bs, 1)\n",
        "                # Add the CLS token to the incoming features\n",
        "                src = torch.cat((CLS, src))\n",
        "\n",
        "                # Adding zeros as the first token in the sequence to be compatible with the CLS token\n",
        "                pos_embed = torch.cat((torch.zeros(1, bs, self.d_model, device=device), pos_embed))\n",
        "\n",
        "                # Adding one mask item to the beginning of the mask to be compatible with CLS token\n",
        "                cls_pad = torch.zeros(bs, 1).bool().to(device)\n",
        "                mask = torch.cat((cls_pad, mask), dim=1)\n",
        "\n",
        "            if self.pass_pos_and_query:\n",
        "                tgt = torch.zeros_like(query_embed)\n",
        "            else:\n",
        "                src, tgt, query_embed, pos_embed = src + 0.1 * pos_embed, query_embed, None, None\n",
        "\n",
        "            device = src.device\n",
        "            if isinstance(text[0], str):\n",
        "                # Encode the text\n",
        "                tokenized = self.tokenizer.batch_encode_plus(text, padding=\"longest\", return_tensors=\"pt\").to(device)\n",
        "                encoded_text = self.text_encoder(**tokenized)\n",
        "\n",
        "                # Transpose memory because pytorch's attention expects sequence first\n",
        "                text_memory = encoded_text.last_hidden_state.transpose(0, 1)\n",
        "                # Invert attention mask that we get from huggingface because its the opposite in pytorch transformer\n",
        "                text_attention_mask = tokenized.attention_mask.ne(1).bool()\n",
        "\n",
        "                # Resize the encoder hidden states to be of the same d_model as the decoder\n",
        "                text_memory_resized = self.resizer(text_memory)\n",
        "            else:\n",
        "                # The text is already encoded, use as is.\n",
        "                text_attention_mask, text_memory_resized, tokenized = text\n",
        "\n",
        "            # Concat on the sequence dimension\n",
        "            src = torch.cat([src, text_memory_resized], dim=0)\n",
        "            # For mask, sequence dimension is second\n",
        "            mask = torch.cat([mask, text_attention_mask], dim=1)\n",
        "            # Pad the pos_embed with 0 so that the addition will be a no-op for the text tokens\n",
        "            pos_embed = torch.cat([pos_embed, torch.zeros_like(text_memory_resized)], dim=0)\n",
        "\n",
        "            img_memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n",
        "\n",
        "            text_memory = img_memory[-len(text_memory_resized) :]\n",
        "\n",
        "            assert img_memory.shape[1] == text_memory.shape[1] == tgt.shape[1]\n",
        "            memory_cache = {\n",
        "                \"text_memory_resized\": text_memory_resized,\n",
        "                \"text_memory\": text_memory,\n",
        "                \"img_memory\": img_memory,\n",
        "                \"text_pooled_op\": encoded_text.pooler_output if self.CLS is not None else None,\n",
        "                \"img_pooled_op\": img_memory[0] if self.CLS is not None else None,  # Return the CLS token\n",
        "                \"mask\": mask,\n",
        "                \"text_attention_mask\": text_attention_mask,\n",
        "                \"pos_embed\": pos_embed,\n",
        "                \"query_embed\": query_embed,\n",
        "                \"tokenized\": tokenized,\n",
        "            }\n",
        "            return memory_cache\n",
        "\n",
        "        else:\n",
        "            if self.pass_pos_and_query:\n",
        "                tgt = torch.zeros_like(query_embed)\n",
        "            else:\n",
        "                src, tgt, query_embed, pos_embed = src + 0.1 * pos_embed, query_embed, None, None\n",
        "\n",
        "            assert img_memory.shape[1] == text_memory.shape[1] == tgt.shape[1]\n",
        "\n",
        "            hs = self.decoder(\n",
        "                tgt,\n",
        "                img_memory,\n",
        "                text_memory,\n",
        "                memory_key_padding_mask=mask,\n",
        "                text_memory_key_padding_mask=text_attention_mask,\n",
        "                pos=pos_embed,\n",
        "                query_pos=query_embed,\n",
        "            )\n",
        "            return hs.transpose(1, 2)\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(encoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src,\n",
        "        mask: Optional[Tensor] = None,\n",
        "        src_key_padding_mask: Optional[Tensor] = None,\n",
        "        pos: Optional[Tensor] = None,\n",
        "    ):\n",
        "\n",
        "        output = src\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask, pos=pos)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n",
        "        super().__init__()\n",
        "        self.layers = _get_clones(decoder_layer, num_layers)\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = norm\n",
        "        self.return_intermediate = return_intermediate\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        tgt,\n",
        "        memory,\n",
        "        text_memory,\n",
        "        tgt_mask: Optional[Tensor] = None,\n",
        "        memory_mask: Optional[Tensor] = None,\n",
        "        text_memory_key_padding_mask: Optional[Tensor] = None,\n",
        "        tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "        memory_key_padding_mask: Optional[Tensor] = None,\n",
        "        pos: Optional[Tensor] = None,\n",
        "        query_pos: Optional[Tensor] = None,\n",
        "    ):\n",
        "        output = tgt\n",
        "\n",
        "        intermediate = []\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(\n",
        "                output,\n",
        "                memory,\n",
        "                text_memory=text_memory,\n",
        "                tgt_mask=tgt_mask,\n",
        "                memory_mask=memory_mask,\n",
        "                text_memory_key_padding_mask=text_memory_key_padding_mask,\n",
        "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                memory_key_padding_mask=memory_key_padding_mask,\n",
        "                pos=pos,\n",
        "                query_pos=query_pos,\n",
        "            )\n",
        "            if self.return_intermediate:\n",
        "                intermediate.append(self.norm(output))\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "            if self.return_intermediate:\n",
        "                intermediate.pop()\n",
        "                intermediate.append(output)\n",
        "\n",
        "        if self.return_intermediate:\n",
        "            return torch.stack(intermediate)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\", normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    def forward_post(\n",
        "        self,\n",
        "        src,\n",
        "        src_mask: Optional[Tensor] = None,\n",
        "        src_key_padding_mask: Optional[Tensor] = None,\n",
        "        pos: Optional[Tensor] = None,\n",
        "    ):\n",
        "        q = k = self.with_pos_embed(src, pos)\n",
        "        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "    def forward_pre(\n",
        "        self,\n",
        "        src,\n",
        "        src_mask: Optional[Tensor] = None,\n",
        "        src_key_padding_mask: Optional[Tensor] = None,\n",
        "        pos: Optional[Tensor] = None,\n",
        "    ):\n",
        "        src2 = self.norm1(src)\n",
        "        q = k = self.with_pos_embed(src2, pos)\n",
        "        src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src2 = self.norm2(src)\n",
        "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        return src\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src,\n",
        "        src_mask: Optional[Tensor] = None,\n",
        "        src_key_padding_mask: Optional[Tensor] = None,\n",
        "        pos: Optional[Tensor] = None,\n",
        "    ):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n",
        "        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
        "\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\", normalize_before=False):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.cross_attn_image = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        # self.cross_attn_text = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "\n",
        "        # Implementation of Feedforward model\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        # self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.norm4 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        # self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "        self.dropout4 = nn.Dropout(dropout)\n",
        "\n",
        "        self.activation = _get_activation_fn(activation)\n",
        "        self.normalize_before = normalize_before\n",
        "\n",
        "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
        "        return tensor if pos is None else tensor + pos\n",
        "\n",
        "    # For now, trying one version where its self attn -> cross attn text -> cross attn image -> FFN\n",
        "    def forward_post(\n",
        "        self,\n",
        "        tgt,\n",
        "        memory,\n",
        "        text_memory,\n",
        "        tgt_mask: Optional[Tensor] = None,\n",
        "        memory_mask: Optional[Tensor] = None,\n",
        "        text_memory_key_padding_mask: Optional[Tensor] = None,\n",
        "        tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "        memory_key_padding_mask: Optional[Tensor] = None,\n",
        "        pos: Optional[Tensor] = None,\n",
        "        query_pos: Optional[Tensor] = None,\n",
        "    ):\n",
        "        q = k = self.with_pos_embed(tgt, query_pos)\n",
        "\n",
        "        # Self attention\n",
        "        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "\n",
        "        # Cross attention to text\n",
        "        # tgt2 = self.cross_attn_text(\n",
        "        #     query=self.with_pos_embed(tgt, query_pos),\n",
        "        #     key=text_memory,\n",
        "        #     value=text_memory,\n",
        "        #     attn_mask=None,\n",
        "        #     key_padding_mask=text_memory_key_padding_mask,\n",
        "        # )[0]\n",
        "        # tgt = tgt + self.dropout2(tgt2)\n",
        "        # tgt = self.norm2(tgt)\n",
        "\n",
        "        # Cross attention to image\n",
        "        tgt2 = self.cross_attn_image(\n",
        "            query=self.with_pos_embed(tgt, query_pos),\n",
        "            key=self.with_pos_embed(memory, pos),\n",
        "            value=memory,\n",
        "            attn_mask=memory_mask,\n",
        "            key_padding_mask=memory_key_padding_mask,\n",
        "        )[0]\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "\n",
        "        # FFN\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout4(tgt2)\n",
        "        tgt = self.norm4(tgt)\n",
        "        return tgt\n",
        "\n",
        "    def forward_pre(\n",
        "        self,\n",
        "        tgt,\n",
        "        memory,\n",
        "        text_memory,\n",
        "        tgt_mask: Optional[Tensor] = None,\n",
        "        memory_mask: Optional[Tensor] = None,\n",
        "        text_memory_key_padding_mask: Optional[Tensor] = None,\n",
        "        tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "        memory_key_padding_mask: Optional[Tensor] = None,\n",
        "        pos: Optional[Tensor] = None,\n",
        "        query_pos: Optional[Tensor] = None,\n",
        "    ):\n",
        "        assert False, \"not implemented yet\"\n",
        "        tgt2 = self.norm1(tgt)\n",
        "        q = k = self.with_pos_embed(tgt2, query_pos)\n",
        "        tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt2 = self.norm2(tgt)\n",
        "        tgt2 = self.multihead_attn(\n",
        "            query=self.with_pos_embed(tgt2, query_pos),\n",
        "            key=self.with_pos_embed(memory, pos),\n",
        "            value=memory,\n",
        "            attn_mask=memory_mask,\n",
        "            key_padding_mask=memory_key_padding_mask,\n",
        "        )[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt2 = self.norm3(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        return tgt\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        tgt,\n",
        "        memory,\n",
        "        text_memory,\n",
        "        tgt_mask: Optional[Tensor] = None,\n",
        "        memory_mask: Optional[Tensor] = None,\n",
        "        text_memory_key_padding_mask: Optional[Tensor] = None,\n",
        "        tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "        memory_key_padding_mask: Optional[Tensor] = None,\n",
        "        pos: Optional[Tensor] = None,\n",
        "        query_pos: Optional[Tensor] = None,\n",
        "    ):\n",
        "        if self.normalize_before:\n",
        "            return self.forward_pre(\n",
        "                tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos\n",
        "            )\n",
        "        return self.forward_post(\n",
        "            tgt,\n",
        "            memory,\n",
        "            text_memory,\n",
        "            tgt_mask,\n",
        "            memory_mask,\n",
        "            text_memory_key_padding_mask,\n",
        "            tgt_key_padding_mask,\n",
        "            memory_key_padding_mask,\n",
        "            pos,\n",
        "            query_pos,\n",
        "        )\n",
        "\n",
        "\n",
        "class FeatureResizer(nn.Module):\n",
        "    \"\"\"\n",
        "    This class takes as input a set of embeddings of dimension C1 and outputs a set of\n",
        "    embedding of dimension C2, after a linear transformation, dropout and normalization (LN).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_feat_size, output_feat_size, dropout, do_ln=True):\n",
        "        super().__init__()\n",
        "        self.do_ln = do_ln\n",
        "        # Object feature encoding\n",
        "        self.fc = nn.Linear(input_feat_size, output_feat_size, bias=True)\n",
        "        self.layer_norm = nn.LayerNorm(output_feat_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, encoder_features):\n",
        "        x = self.fc(encoder_features)\n",
        "        if self.do_ln:\n",
        "            x = self.layer_norm(x)\n",
        "        output = self.dropout(x)\n",
        "        return output\n",
        "\n",
        "\n",
        "def _get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "\n",
        "def build_transformer(args):\n",
        "    return Transformer(\n",
        "        d_model=args.hidden_dim,\n",
        "        dropout=args.dropout,\n",
        "        nhead=args.nheads,\n",
        "        dim_feedforward=args.dim_feedforward,\n",
        "        num_encoder_layers=args.enc_layers,\n",
        "        num_decoder_layers=args.dec_layers,\n",
        "        normalize_before=args.pre_norm,\n",
        "        return_intermediate_dec=True,\n",
        "        pass_pos_and_query=args.pass_pos_and_query,\n",
        "        text_encoder_type=args.text_encoder_type,\n",
        "        freeze_text_encoder=args.freeze_text_encoder,\n",
        "        contrastive_loss=args.contrastive_loss,\n",
        "    )\n",
        "\n",
        "\n",
        "def _get_activation_fn(activation):\n",
        "    \"\"\"Return an activation function given a string\"\"\"\n",
        "    if activation == \"relu\":\n",
        "        return F.relu\n",
        "    if activation == \"gelu\":\n",
        "        return F.gelu\n",
        "    if activation == \"glu\":\n",
        "        return F.glu\n",
        "    raise RuntimeError(f\"activation should be relu/gelu, not {activation}.\")"
      ],
      "metadata": {
        "id": "1s3IOvMfVGok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "N2Q2xeJmV8bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "n_v_XZ5rTqQ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}